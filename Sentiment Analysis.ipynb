{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP 3 Abdelhadi_Zahir.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sDQRUmzkKAO6",
        "84OqeBsjhUvb",
        "Ie90TWOXuW8d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOWQfHFmppUL"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\r\n",
        "\r\n",
        "from sklearn.preprocessing import MinMaxScaler    \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RAinoHKWjbk"
      },
      "source": [
        "# Importation des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bydUT4uVtlTg"
      },
      "source": [
        "from google.colab import files\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXYbfApRzqAP"
      },
      "source": [
        "J'ai renommé le titre des colonnes dans chaque dataframe avec text et label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrlcfsGwtT8Q"
      },
      "source": [
        "amazon_train=pd.read_csv(\"amazon_train.txt\", sep= '\\t')\r\n",
        "amazon_test=pd.read_csv(\"amazon_test.txt\", sep= '\\t')\r\n",
        "amazon_dev=pd.read_csv(\"amazon_dev.txt\", sep= '\\t')\r\n",
        "imdb_train=pd.read_csv(\"imdb_train.txt\", sep= '\\t')\r\n",
        "imdb_test=pd.read_csv(\"imdb_test.txt\", sep= '\\t')\r\n",
        "imdb_dev=pd.read_csv(\"imdb_dev.txt\", sep= '\\t')\r\n",
        "yelp_train=pd.read_csv(\"yelp_train.txt\", sep= '\\t')\r\n",
        "yelp_test=pd.read_csv(\"yelp_test.txt\", sep= '\\t')\r\n",
        "yelp_dev=pd.read_csv(\"yelp_dev.txt\", sep= '\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "rNS3HtHDuPN5",
        "outputId": "219e8386-f541-4c89-ad48-555ded7cc773"
      },
      "source": [
        "yelp_dev.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The fries were great too.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A great touch.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0                           Wow... Loved this place.      1\n",
              "1  Stopped by during the late May bank holiday of...      1\n",
              "2  The selection on the menu was great and so wer...      1\n",
              "3                          The fries were great too.      1\n",
              "4                                     A great touch.      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmBT7HdNWp60"
      },
      "source": [
        "## Prétraitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwm9NIs_tVKm"
      },
      "source": [
        "J'ai essayé d'ensembler tous les fichiers train/dev/test dans un seul pour faire le prétraitement une seul fois pour ne pas refaire beaucoup de travail,  mais j'ai trouvé problème pour le lire, donc j'ai fait le prétraitement pour chaque fichier à part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKtiQ146Y3uG"
      },
      "source": [
        "Elimination des majuscules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LC-soL0uwwp"
      },
      "source": [
        "amazon_train['text']=amazon_train['text'].map(lambda x: x.lower())\r\n",
        "amazon_test['text']=amazon_test['text'].map(lambda x: x.lower())\r\n",
        "amazon_dev['text']=amazon_dev['text'].map(lambda x: x.lower())\r\n",
        "imdb_train['text']=imdb_train['text'].map(lambda x: x.lower())\r\n",
        "imdb_test['text']=imdb_test['text'].map(lambda x: x.lower())\r\n",
        "imdb_dev['text']=imdb_dev['text'].map(lambda x: x.lower())\r\n",
        "yelp_train['text']=yelp_train['text'].map(lambda x: x.lower())\r\n",
        "yelp_test['text']=yelp_test['text'].map(lambda x: x.lower())\r\n",
        "yelp_dev['text']=yelp_dev['text'].map(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "B5hwHE2oyW_H",
        "outputId": "1cf97b10-d348-44fc-d4b9-97809dcabbd2"
      },
      "source": [
        "amazon_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this phone is pretty sturdy and i've never had...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i love this thing!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>everything about it is fine and reasonable for...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i even dropped this phone into a stream and it...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i have been very happy with the 510 and have h...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  this phone is pretty sturdy and i've never had...      1\n",
              "1                                 i love this thing!      1\n",
              "2  everything about it is fine and reasonable for...      1\n",
              "3  i even dropped this phone into a stream and it...      1\n",
              "4  i have been very happy with the 510 and have h...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycne_mmcxi73"
      },
      "source": [
        "Elimination de la ponctutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2MYcOx4oxs"
      },
      "source": [
        "import re\r\n",
        "def  r_punctuation(text):\r\n",
        "    \"\"\" Return a cleaned version of text\r\n",
        "    \"\"\"\r\n",
        "    text = re.sub('<[^>]*>', '', text)\r\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\r\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\r\n",
        "    \r\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lr-aO7EazFB"
      },
      "source": [
        "amazon_train['text'] = amazon_train['text'].apply(r_punctuation)\r\n",
        "amazon_dev['text'] = amazon_dev['text'].apply(r_punctuation)\r\n",
        "amazon_test['text'] = amazon_test['text'].apply(r_punctuation)\r\n",
        "imdb_train['text'] = imdb_train['text'].apply(r_punctuation)\r\n",
        "imdb_dev['text'] = imdb_dev['text'].apply(r_punctuation)\r\n",
        "imdb_test['text'] = imdb_test['text'].apply(r_punctuation)\r\n",
        "yelp_train['text'] = yelp_train['text'].apply(r_punctuation)\r\n",
        "yelp_test['text'] = yelp_test['text'].apply(r_punctuation)\r\n",
        "yelp_dev['text'] = yelp_dev['text'].apply(r_punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAaV_sYfZGPl"
      },
      "source": [
        "text_amazon_train = ''.join([c for c in amazon_train['text']])\r\n",
        "text_amazon_test = ''.join([c for c in amazon_test['text']])\r\n",
        "text_amazon_dev = ''.join([c for c in amazon_dev['text']])\r\n",
        "text_imdb_train = ''.join([c for c in imdb_train['text']])\r\n",
        "text_imdb_test = ''.join([c for c in imdb_test['text']])\r\n",
        "text_imdb_dev = ''.join([c for c in imdb_dev['text']])\r\n",
        "text_yelp_train = ''.join([c for c in yelp_train['text']])\r\n",
        "text_yelp_test = ''.join([c for c in yelp_test['text']])\r\n",
        "text_yelp_dev = ''.join([c for c in yelp_dev['text']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94b3eD71n9yD"
      },
      "source": [
        "Tokénisation et création du vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuewKw1FPauT"
      },
      "source": [
        "text_split1 = [c for c in amazon_train['text']]\r\n",
        "text_split2 = [c for c in amazon_test['text']]\r\n",
        "text_split3 = [c for c in amazon_dev['text']]\r\n",
        "text_split4 = [c for c in imdb_train['text']]\r\n",
        "text_split5 = [c for c in imdb_test['text']]\r\n",
        "text_split6 = [c for c in imdb_dev['text']]\r\n",
        "text_split7 = [c for c in yelp_train['text']]\r\n",
        "text_split8 = [c for c in yelp_test['text']]\r\n",
        "text_split9 = [c for c in yelp_dev['text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9InrxDUc_2W"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NR6EeMwuxGd"
      },
      "source": [
        "text_amazon_train = ' '.join(text_split1)\r\n",
        "words1 = text_amazon_train.split()\r\n",
        "\r\n",
        "text_amazon_test = ' '.join(text_split2)\r\n",
        "words1.extend(text_amazon_test.split())\r\n",
        "\r\n",
        "text_amazon_dev = ' '.join(text_split3)\r\n",
        "words1.extend(text_amazon_dev.split())\r\n",
        "\r\n",
        "text_imdb_train = ' '.join(text_split4)\r\n",
        "words2 = text_imdb_train.split()\r\n",
        "\r\n",
        "text_imdb_test = ' '.join(text_split5)\r\n",
        "words2.extend(text_imdb_test.split())\r\n",
        "\r\n",
        "text_imdb_dev = ' '.join(text_split6)\r\n",
        "words2.extend(text_imdb_dev.split())\r\n",
        "\r\n",
        "text_yelp_train = ' '.join(text_split7)\r\n",
        "words3 = text_yelp_train.split()\r\n",
        "\r\n",
        "text_yelp_test = ' '.join(text_split8)\r\n",
        "words3.extend(text_yelp_test.split())\r\n",
        "\r\n",
        "text_yelp_dev = ' '.join(text_split9)\r\n",
        "words3.extend(text_yelp_dev.split())\r\n",
        "\r\n",
        "count_words = Counter(words1)\r\n",
        "total_words = len(words1)\r\n",
        "sorted_words = count_words.most_common(total_words)\r\n",
        "\r\n",
        "count_words2 = Counter(words2)\r\n",
        "total_words2 = len(words2)\r\n",
        "sorted_words2 = count_words2.most_common(total_words2)\r\n",
        "\r\n",
        "count_words3 = Counter(words3)\r\n",
        "total_words3 = len(words3)\r\n",
        "sorted_words3 = count_words3.most_common(total_words3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN0OwjjJdMTe"
      },
      "source": [
        "vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}\r\n",
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\r\n",
        "\r\n",
        "vocab_to_int2 = {w:i for i, (w,c) in enumerate(sorted_words2)}\r\n",
        "vocab_to_int2 = {w:i+1 for i, (w,c) in enumerate(sorted_words2)}\r\n",
        "\r\n",
        "vocab_to_int3 = {w:i for i, (w,c) in enumerate(sorted_words3)}\r\n",
        "vocab_to_int3 = {w:i+1 for i, (w,c) in enumerate(sorted_words3)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xifvtoPdiT-"
      },
      "source": [
        "ama_train_int = []\r\n",
        "for review in text_split1:\r\n",
        "  r = [vocab_to_int[w] for w in review.split()]\r\n",
        "  ama_train_int.append(r)\r\n",
        "\r\n",
        "ama_test_int = []\r\n",
        "for review in text_split2:\r\n",
        "  r = [vocab_to_int[w] for w in review.split()]\r\n",
        "  ama_test_int.append(r)\r\n",
        "\r\n",
        "ama_dev_int = []\r\n",
        "for review in text_split3:\r\n",
        "  r = [vocab_to_int[w] for w in review.split()]\r\n",
        "  ama_dev_int.append(r)\r\n",
        "\r\n",
        "imdb_train_int = []\r\n",
        "for review in text_split4:\r\n",
        "  r = [vocab_to_int2[w] for w in review.split()]\r\n",
        "  imdb_train_int.append(r)\r\n",
        "\r\n",
        "\r\n",
        "imdb_test_int = []\r\n",
        "for review in text_split5:\r\n",
        "  r = [vocab_to_int2[w] for w in review.split()]\r\n",
        "  imdb_test_int.append(r)\r\n",
        "\r\n",
        "\r\n",
        "imdb_dev_int = []\r\n",
        "for review in text_split6:\r\n",
        "  r = [vocab_to_int2[w] for w in review.split()]\r\n",
        "  imdb_dev_int.append(r)\r\n",
        "\r\n",
        "\r\n",
        "yelp_train_int = []\r\n",
        "for review in text_split7:\r\n",
        "  r = [vocab_to_int3[w] for w in review.split()]\r\n",
        "  yelp_train_int.append(r)\r\n",
        "\r\n",
        "yelp_test_int = []\r\n",
        "for review in text_split8:\r\n",
        "  r = [vocab_to_int3[w] for w in review.split()]\r\n",
        "  yelp_test_int.append(r)\r\n",
        "\r\n",
        "\r\n",
        "yelp_dev_int = []\r\n",
        "for review in text_split9:\r\n",
        "  r = [vocab_to_int3[w] for w in review.split()]\r\n",
        "  yelp_dev_int.append(r)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VlJvM-CdojB"
      },
      "source": [
        "ama_train_label = [1 if label ==1 else 0 for label in amazon_train['label']]\r\n",
        "ama_train_label = np.array(ama_train_label)\r\n",
        "\r\n",
        "ama_test_label = [1 if label ==1 else 0 for label in amazon_test['label']]\r\n",
        "ama_test_label = np.array(ama_test_label)\r\n",
        "\r\n",
        "ama_dev_label = [1 if label ==1 else 0 for label in amazon_dev['label']]\r\n",
        "ama_dev_label = np.array(ama_dev_label)\r\n",
        "\r\n",
        "imdb_train_label = [1 if label ==1 else 0 for label in imdb_train['label']]\r\n",
        "imdb_train_label = np.array(imdb_train_label)\r\n",
        "\r\n",
        "imdb_test_label = [1 if label ==1 else 0 for label in imdb_test['label']]\r\n",
        "imdb_test_label = np.array(imdb_test_label)\r\n",
        "\r\n",
        "imdb_dev_label = [1 if label ==1 else 0 for label in imdb_dev['label']]\r\n",
        "imdb_dev_label = np.array(imdb_dev_label)\r\n",
        "\r\n",
        "yelp_train_label = [1 if label ==1 else 0 for label in yelp_train['label']]\r\n",
        "yelp_train_label = np.array(yelp_train_label)\r\n",
        "\r\n",
        "yelp_test_label = [1 if label ==1 else 0 for label in yelp_test['label']]\r\n",
        "yelp_test_label = np.array(yelp_test_label)\r\n",
        "\r\n",
        "yelp_dev_label = [1 if label ==1 else 0 for label in yelp_dev['label']]\r\n",
        "yelp_dev_label = np.array(yelp_dev_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh42_D-wefwA"
      },
      "source": [
        "ama_train_len = [len(x) for x in ama_train_int]\r\n",
        "ama_test_len = [len(x) for x in ama_test_int]\r\n",
        "ama_dev_len = [len(x) for x in ama_dev_int]\r\n",
        "imdb_train_len = [len(x) for x in imdb_train_int]\r\n",
        "imdb_test_len = [len(x) for x in imdb_test_int]\r\n",
        "imdb_dev_len = [len(x) for x in imdb_dev_int]\r\n",
        "yelp_train_len = [len(x) for x in yelp_train_int]\r\n",
        "yelp_test_len = [len(x) for x in yelp_test_int]\r\n",
        "yelp_dev_len = [len(x) for x in yelp_dev_int]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XBoGEKle0lK"
      },
      "source": [
        "ama_train_int = [ ama_train_int[i] for i, l in enumerate(ama_train_len) if l>0 ]\r\n",
        "Y_ama_train = [ ama_train_label[i] for i, l in enumerate(ama_train_len) if l> 0 ]\r\n",
        "\r\n",
        "ama_test_int = [ ama_test_int[i] for i, l in enumerate(ama_test_len) if l>0 ]\r\n",
        "Y_ama_test = [ ama_test_label[i] for i, l in enumerate(ama_test_len) if l> 0 ]\r\n",
        "\r\n",
        "ama_dev_int = [ ama_dev_int[i] for i, l in enumerate(ama_dev_len) if l>0 ]\r\n",
        "Y_ama_dev = [ ama_dev_label[i] for i, l in enumerate(ama_dev_len) if l> 0 ]\r\n",
        "\r\n",
        "imdb_train_int = [ imdb_train_int[i] for i, l in enumerate(imdb_train_len) if l>0 ]\r\n",
        "Y_imdb_train = [ imdb_train_label[i] for i, l in enumerate(imdb_train_len) if l> 0 ]\r\n",
        "\r\n",
        "imdb_test_int = [ imdb_test_int[i] for i, l in enumerate(imdb_test_len) if l>0 ]\r\n",
        "Y_imdb_test = [ imdb_test_label[i] for i, l in enumerate(imdb_test_len) if l> 0 ]\r\n",
        "\r\n",
        "imdb_dev_int = [ imdb_dev_int[i] for i, l in enumerate(imdb_dev_len) if l>0 ]\r\n",
        "Y_imdb_dev = [ imdb_dev_label[i] for i, l in enumerate(imdb_dev_len) if l> 0 ]\r\n",
        "\r\n",
        "yelp_train_int = [ yelp_train_int[i] for i, l in enumerate(yelp_train_len) if l>0 ]\r\n",
        "Y_yelp_train = [ yelp_train_label[i] for i, l in enumerate(yelp_train_len) if l> 0 ]\r\n",
        "\r\n",
        "yelp_test_int = [ yelp_test_int[i] for i, l in enumerate(yelp_test_len) if l>0 ]\r\n",
        "Y_yelp_test = [ yelp_test_label[i] for i, l in enumerate(yelp_test_len) if l> 0 ]\r\n",
        "\r\n",
        "yelp_dev_int = [ yelp_dev_int[i] for i, l in enumerate(yelp_dev_len) if l>0 ]\r\n",
        "Y_yelp_dev = [ yelp_dev_label[i] for i, l in enumerate(yelp_dev_len) if l> 0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhU1bb3fe3CL"
      },
      "source": [
        "def pad_features(reviews_int, seq_length):\r\n",
        "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\r\n",
        "    '''\r\n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\r\n",
        "    \r\n",
        "    for i, review in enumerate(reviews_int):\r\n",
        "        review_len = len(review)\r\n",
        "        \r\n",
        "        if review_len <= seq_length:\r\n",
        "            zeroes = list(np.zeros(seq_length-review_len))\r\n",
        "            new = zeroes+review\r\n",
        "        elif review_len > seq_length:\r\n",
        "            new = review[0:seq_length]\r\n",
        "        \r\n",
        "        features[i,:] = np.array(new)\r\n",
        "    \r\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwWJWBPEkoqK"
      },
      "source": [
        "seq_length=200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8UrAffSjF93"
      },
      "source": [
        "X_ama_train = pad_features(ama_train_int, seq_length)\r\n",
        "X_ama_test = pad_features(ama_test_int, seq_length)\r\n",
        "X_ama_dev = pad_features(ama_dev_int, seq_length)\r\n",
        "X_imdb_train = pad_features(imdb_train_int, seq_length)\r\n",
        "X_imdb_test = pad_features(imdb_test_int, seq_length)\r\n",
        "X_imdb_dev = pad_features(imdb_dev_int, seq_length)\r\n",
        "X_yelp_train = pad_features(yelp_train_int, seq_length)\r\n",
        "X_yelp_test = pad_features(yelp_test_int, seq_length)\r\n",
        "X_yelp_dev = pad_features(yelp_dev_int, seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g6Q38pxin4V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3y6cfY-t9zd"
      },
      "source": [
        "# Modèle 1 : Pour amazon dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9g0mMaw2rBt"
      },
      "source": [
        "Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbFJNenpuC1Q"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import DataLoader, TensorDataset\r\n",
        "# create Tensor datasets\r\n",
        "train_data = TensorDataset(torch.from_numpy(np.array(X_ama_train)), torch.from_numpy(np.array(Y_ama_train)))\r\n",
        "valid_data = TensorDataset(torch.from_numpy(np.array(X_ama_dev)), torch.from_numpy(np.array(Y_ama_dev)))\r\n",
        "test_data = TensorDataset(torch.from_numpy(np.array(X_ama_test)), torch.from_numpy(np.array(Y_ama_test)))\r\n",
        "# dataloaders\r\n",
        "batch_size = 50\r\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\r\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\r\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrcT4UbQ5iwu",
        "outputId": "795c1bae-1188-4f86-aa26-ffb9dd7cc47e"
      },
      "source": [
        "# obtain one batch of training data\r\n",
        "dataiter = iter(train_loader)\r\n",
        "sample_x, sample_y = dataiter.next()\r\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\r\n",
        "print('Sample input: \\n', sample_x)\r\n",
        "print()\r\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\r\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[  0,   0,   0,  ..., 727,  11,  85],\n",
            "        [  0,   0,   0,  ...,  45,   1, 467],\n",
            "        [  0,   0,   0,  ...,   1, 671, 535],\n",
            "        ...,\n",
            "        [  0,   0,   0,  ...,  18,   1,   9],\n",
            "        [  0,   0,   0,  ...,  22,  31, 142],\n",
            "        [  0,   0,   0,  ...,   0,   6, 331]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V_h53e3-rEn"
      },
      "source": [
        "Créatiion du modèle.\r\n",
        "\r\n",
        "\r\n",
        "Le modèle contient :\r\n",
        "- une couche pour conversion de nos mots en tokens\r\n",
        "- une couche qui convertit les tokens en embeddings de taille spécifique\r\n",
        "- couche cachée de LSTM\r\n",
        "- une dernière couche d'activation sigmoïde, qui transforme toutes les valeurs de sortie en une valeur comprise entre 0 et 1\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rxJhrzK9JWu"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "class SentimentLSTM(nn.Module):\r\n",
        "    \r\n",
        "\r\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.output_size = output_size\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.hidden_dim = hidden_dim\r\n",
        "        \r\n",
        "        # embedding and LSTM layers\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \r\n",
        "                            dropout=drop_prob, batch_first=True)\r\n",
        "        \r\n",
        "        # dropout layer\r\n",
        "        self.dropout = nn.Dropout(0.3)\r\n",
        "        \r\n",
        "        # linear and sigmoid layers\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\r\n",
        "        self.sig = nn.Sigmoid()\r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, x, hidden):\r\n",
        "        \r\n",
        "        batch_size = x.size(0)\r\n",
        "\r\n",
        "        # embeddings and lstm_out\r\n",
        "        embeds = self.embedding(x)\r\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\r\n",
        "    \r\n",
        "        # stack up lstm outputs\r\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\r\n",
        "        \r\n",
        "        # dropout and fully-connected layer\r\n",
        "        out = self.dropout(lstm_out)\r\n",
        "        out = self.fc(out)\r\n",
        "        # sigmoid function\r\n",
        "        sig_out = self.sig(out)\r\n",
        "        \r\n",
        "        # reshape to be batch_size first\r\n",
        "        sig_out = sig_out.view(batch_size, -1)\r\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\r\n",
        "        \r\n",
        "        # return last sigmoid output and hidden state\r\n",
        "        return sig_out, hidden\r\n",
        "    \r\n",
        "    \r\n",
        "    def init_hidden(self, batch_size):\r\n",
        "        \r\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\r\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\r\n",
        "        weight = next(self.parameters()).data\r\n",
        "        \r\n",
        "        if (train_on_gpu):\r\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\r\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\r\n",
        "        else:\r\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\r\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\r\n",
        "        \r\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV_HKsqk-666"
      },
      "source": [
        "Entrainement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnX4AYZP-lhT",
        "outputId": "6b543936-fbd4-4b4d-b587-566eeacc9c1c"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\r\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\r\n",
        "output_size = 1\r\n",
        "embedding_dim = 400\r\n",
        "hidden_dim = 256\r\n",
        "n_layers = 2\r\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\r\n",
        "print(net)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(1867, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-m3Mxbf8fQJ",
        "outputId": "1b33da2a-3d37-4175-a1fa-096f8ad7d8b7"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-ybxCco_ufe"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erqgyNMc-9zj"
      },
      "source": [
        "# loss and optimization functions\r\n",
        "lr=0.001\r\n",
        "\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\r\n",
        "\r\n",
        "\r\n",
        "# training params\r\n",
        "\r\n",
        "epochs = 4 \r\n",
        "\r\n",
        "counter = 0\r\n",
        "print_every = 100\r\n",
        "clip=5 # gradient clipping\r\n",
        "\r\n",
        "# move model to GPU, if available\r\n",
        "if(train_on_gpu):\r\n",
        "    net.cuda()\r\n",
        "\r\n",
        "net.train()\r\n",
        "# train for some number of epochs\r\n",
        "for e in range(epochs):\r\n",
        "    # initialize hidden state\r\n",
        "    h = net.init_hidden(batch_size)\r\n",
        "\r\n",
        "    # batch loop\r\n",
        "    for inputs, labels in train_loader:\r\n",
        "        counter += 1\r\n",
        "\r\n",
        "        if(train_on_gpu):\r\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "\r\n",
        "        \r\n",
        "        h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "        # zero accumulated gradients\r\n",
        "        net.zero_grad()\r\n",
        "\r\n",
        "        # get the output from the model\r\n",
        "        inputs = inputs.type(torch.LongTensor)\r\n",
        "        inputs = inputs.cuda()\r\n",
        "        output, h = net(inputs, h)\r\n",
        "\r\n",
        "        # calculate the loss and perform backprop\r\n",
        "        loss = criterion(output.squeeze(), labels.float())\r\n",
        "        loss.backward()\r\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # loss stats\r\n",
        "        if counter % print_every == 0:\r\n",
        "            # Get validation loss\r\n",
        "            val_h = net.init_hidden(batch_size)\r\n",
        "            val_losses = []\r\n",
        "            net.eval()\r\n",
        "            for inputs, labels in valid_loader:\r\n",
        "\r\n",
        "                \r\n",
        "                val_h = tuple([each.data for each in val_h])\r\n",
        "\r\n",
        "                if(train_on_gpu):\r\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "\r\n",
        "                inputs = inputs.type(torch.LongTensor)\r\n",
        "                output, val_h = net(inputs, val_h)\r\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\r\n",
        "\r\n",
        "                val_losses.append(val_loss.item())\r\n",
        "\r\n",
        "            net.train()\r\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\r\n",
        "                  \"Step: {}...\".format(counter),\r\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\r\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yirkZm6TBOal"
      },
      "source": [
        "Test du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U77JzeMM-Zxc",
        "outputId": "97ede712-6586-4d0b-bff9-2c0b0d8c1174"
      },
      "source": [
        "# Get test data loss and accuracy\r\n",
        "\r\n",
        "test_losses = [] # track loss\r\n",
        "num_correct = 0\r\n",
        "\r\n",
        "# init hidden state\r\n",
        "h = net.init_hidden(batch_size)\r\n",
        "\r\n",
        "net.eval()\r\n",
        "# iterate over test data\r\n",
        "for inputs, labels in test_loader:\r\n",
        "\r\n",
        "    \r\n",
        "    h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "    if(train_on_gpu):\r\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "    \r\n",
        "    # get predicted outputs\r\n",
        "    inputs = inputs.type(torch.LongTensor)\r\n",
        "    inputs = inputs.cuda()\r\n",
        "    output, h = net(inputs, h)\r\n",
        "    \r\n",
        "    # calculate loss\r\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\r\n",
        "    test_losses.append(test_loss.item())\r\n",
        "    \r\n",
        "    # convert output probabilities to predicted class (0 or 1)\r\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\r\n",
        "    \r\n",
        "    # compare predictions to true label\r\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\r\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\r\n",
        "    num_correct += np.sum(correct)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# accuracy over all test data\r\n",
        "test_acc = num_correct/len(test_loader.dataset)\r\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzxL2Hn6CRcd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDQRUmzkKAO6"
      },
      "source": [
        "# Modèle 2 : Pour imdb dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf_qobfDKGH1"
      },
      "source": [
        "# create Tensor datasets\r\n",
        "train_data2 = TensorDataset(torch.from_numpy(np.array(X_imdb_train)), torch.from_numpy(np.array(Y_imdb_train)))\r\n",
        "valid_data2 = TensorDataset(torch.from_numpy(np.array(X_imdb_dev)), torch.from_numpy(np.array(Y_imdb_dev)))\r\n",
        "test_data2 = TensorDataset(torch.from_numpy(np.array(X_imdb_test)), torch.from_numpy(np.array(Y_imdb_test)))\r\n",
        "# dataloaders\r\n",
        "batch_size = 2\r\n",
        "train_loader2 = DataLoader(train_data2, shuffle=True, batch_size=batch_size)\r\n",
        "valid_loader2 = DataLoader(valid_data2, shuffle=True, batch_size=batch_size)\r\n",
        "test_loader2 = DataLoader(test_data2, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgMvcHf9MYgb"
      },
      "source": [
        "# obtain one batch of training data\r\n",
        "dataiter2 = iter(train_loader2)\r\n",
        "sample_x2, sample_y2 = dataiter2.next()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIAupRtwO_EP"
      },
      "source": [
        "Entrainement du modèle 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWH7p9HhMoVT",
        "outputId": "9def7976-c03b-40a1-caf4-d121812418ea"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\r\n",
        "vocab_size2 = len(vocab_to_int2)+1 # +1 for the 0 padding\r\n",
        "net2 = SentimentLSTM(vocab_size2, output_size, embedding_dim, hidden_dim, n_layers)\r\n",
        "print(net2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(3077, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ztZ9gwAMoXU",
        "outputId": "f18c01e4-b011-4fc6-ce5c-71845f013e97"
      },
      "source": [
        "# loss and optimization functions\r\n",
        "lr=0.001\r\n",
        "\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer2 = torch.optim.Adam(net2.parameters(), lr=lr)\r\n",
        "\r\n",
        "epochs = 4 \r\n",
        "\r\n",
        "counter = 0\r\n",
        "print_every = 100\r\n",
        "clip=5 \r\n",
        "\r\n",
        "\r\n",
        "if(train_on_gpu):\r\n",
        "    net2.cuda()\r\n",
        "\r\n",
        "net2.train()\r\n",
        "\r\n",
        "for e in range(epochs):\r\n",
        "    \r\n",
        "    h = net2.init_hidden(batch_size)\r\n",
        "\r\n",
        "    for inputs, labels in train_loader2:\r\n",
        "        counter += 1\r\n",
        "\r\n",
        "        if(train_on_gpu):\r\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "\r\n",
        "        h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "        net2.zero_grad()\r\n",
        "\r\n",
        "        inputs = inputs.type(torch.LongTensor)\r\n",
        "        inputs = inputs.cuda()\r\n",
        "        output, h = net2(inputs, h)\r\n",
        "\r\n",
        "        loss = criterion(output.squeeze(), labels.float())\r\n",
        "        loss.backward()\r\n",
        "        nn.utils.clip_grad_norm_(net2.parameters(), clip)\r\n",
        "        optimizer2.step()\r\n",
        "\r\n",
        "        if counter % print_every == 0:\r\n",
        "            val_h = net2.init_hidden(batch_size)\r\n",
        "            val_losses = []\r\n",
        "            net2.eval()\r\n",
        "            for inputs, labels in valid_loader2:\r\n",
        "                val_h = tuple([each.data for each in val_h])\r\n",
        "\r\n",
        "                if(train_on_gpu):\r\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "\r\n",
        "                inputs = inputs.type(torch.LongTensor)\r\n",
        "                inputs = inputs.cuda()\r\n",
        "                output, val_h = net2(inputs, val_h)\r\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\r\n",
        "\r\n",
        "                val_losses.append(val_loss.item())\r\n",
        "\r\n",
        "            net2.train()\r\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\r\n",
        "                  \"Step: {}...\".format(counter),\r\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\r\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.901616... Val Loss: 0.701811\n",
            "Epoch: 1/4... Step: 200... Loss: 0.664526... Val Loss: 0.733139\n",
            "Epoch: 1/4... Step: 300... Loss: 0.809648... Val Loss: 0.725327\n",
            "Epoch: 1/4... Step: 400... Loss: 0.223608... Val Loss: 0.723979\n",
            "Epoch: 2/4... Step: 500... Loss: 0.246920... Val Loss: 0.837024\n",
            "Epoch: 2/4... Step: 600... Loss: 0.148009... Val Loss: 0.903636\n",
            "Epoch: 2/4... Step: 700... Loss: 0.053076... Val Loss: 0.793303\n",
            "Epoch: 2/4... Step: 800... Loss: 0.296860... Val Loss: 0.744965\n",
            "Epoch: 3/4... Step: 900... Loss: 0.008704... Val Loss: 1.026233\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.009821... Val Loss: 1.064687\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.129862... Val Loss: 1.409217\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.060528... Val Loss: 1.324847\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.037490... Val Loss: 1.356025\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.003150... Val Loss: 1.405396\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.018738... Val Loss: 1.501619\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.001416... Val Loss: 1.425423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_KpI2j0kEcu"
      },
      "source": [
        "Test du modèle 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tKAY-rfMobF",
        "outputId": "0e7bea15-5232-4c19-b138-d06d81de2b7d"
      },
      "source": [
        "# Get test data loss and accuracy\r\n",
        "\r\n",
        "test_losses = [] # track loss\r\n",
        "num_correct = 0\r\n",
        "\r\n",
        "# init hidden state\r\n",
        "h = net2.init_hidden(batch_size)\r\n",
        "\r\n",
        "net2.eval()\r\n",
        "# iterate over test data\r\n",
        "for inputs, labels in test_loader2:\r\n",
        "\r\n",
        "    h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "    if(train_on_gpu):\r\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "    \r\n",
        "    # get predicted outputs\r\n",
        "    inputs = inputs.type(torch.LongTensor)\r\n",
        "    inputs = inputs.cuda()\r\n",
        "    output, h = net2(inputs, h)\r\n",
        "    \r\n",
        "    # calculate loss\r\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\r\n",
        "    test_losses.append(test_loss.item())\r\n",
        "    \r\n",
        "    # convert output probabilities to predicted class (0 or 1)\r\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\r\n",
        "    \r\n",
        "    # compare predictions to true label\r\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\r\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\r\n",
        "    num_correct += np.sum(correct)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# accuracy over all test data\r\n",
        "test_acc = num_correct/len(test_loader.dataset)\r\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNqz1Ya9bx4z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84OqeBsjhUvb"
      },
      "source": [
        "# Modèle 3 : Pour yelp dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukEQxvSvh9Lz"
      },
      "source": [
        "# create Tensor datasets\r\n",
        "train_data3 = TensorDataset(torch.from_numpy(np.array(X_yelp_train)), torch.from_numpy(np.array(Y_yelp_train)))\r\n",
        "valid_data3 = TensorDataset(torch.from_numpy(np.array(X_yelp_dev)), torch.from_numpy(np.array(Y_yelp_dev)))\r\n",
        "test_data3 = TensorDataset(torch.from_numpy(np.array(X_yelp_test)), torch.from_numpy(np.array(Y_yelp_test)))\r\n",
        "# dataloaders\r\n",
        "batch_size = 50\r\n",
        "train_loader3 = DataLoader(train_data3, shuffle=True, batch_size=batch_size)\r\n",
        "valid_loader3 = DataLoader(valid_data3, shuffle=True, batch_size=batch_size)\r\n",
        "test_loader3 = DataLoader(test_data3, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXAmlTnSiiud"
      },
      "source": [
        "# obtain one batch of training data\r\n",
        "dataiter3 = iter(train_loader3)\r\n",
        "sample_x3, sample_y3 = dataiter3.next()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h5MZRJ5jLKn",
        "outputId": "f59f78a2-1a6f-4bbe-b4e8-be38e8a0d836"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\r\n",
        "vocab_size3 = len(vocab_to_int3)+1 # +1 for the 0 padding\r\n",
        "net3 = SentimentLSTM(vocab_size3, output_size, embedding_dim, hidden_dim, n_layers)\r\n",
        "print(net3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(2052, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3o4R8yJjaxm"
      },
      "source": [
        "# loss and optimization functions\r\n",
        "lr=0.001\r\n",
        "\r\n",
        "criterion = nn.BCELoss()\r\n",
        "optimizer3 = torch.optim.Adam(net3.parameters(), lr=lr)\r\n",
        "\r\n",
        "\r\n",
        "# training params\r\n",
        "\r\n",
        "epochs = 4 \r\n",
        "\r\n",
        "counter = 0\r\n",
        "print_every = 100\r\n",
        "clip=5 # gradient clipping\r\n",
        "\r\n",
        "# move model to GPU, if available\r\n",
        "if(train_on_gpu):\r\n",
        "    net3.cuda()\r\n",
        "\r\n",
        "net3.train()\r\n",
        "# train for some number of epochs\r\n",
        "for e in range(epochs):\r\n",
        "    # initialize hidden state\r\n",
        "    h = net3.init_hidden(batch_size)\r\n",
        "\r\n",
        "    # batch loop\r\n",
        "    for inputs, labels in train_loader3:\r\n",
        "        counter += 1\r\n",
        "\r\n",
        "        if(train_on_gpu):\r\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "\r\n",
        "        h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "        # zero accumulated gradients\r\n",
        "        net3.zero_grad()\r\n",
        "\r\n",
        "        # get the output from the model\r\n",
        "        inputs = inputs.type(torch.LongTensor)\r\n",
        "        inputs = inputs.cuda()\r\n",
        "        output, h = net3(inputs, h)\r\n",
        "\r\n",
        "        # calculate the loss and perform backprop\r\n",
        "        loss = criterion(output.squeeze(), labels.float())\r\n",
        "        loss.backward()\r\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\r\n",
        "        nn.utils.clip_grad_norm_(net3.parameters(), clip)\r\n",
        "        optimizer2.step()\r\n",
        "\r\n",
        "        # loss stats\r\n",
        "        if counter % print_every == 0:\r\n",
        "            # Get validation loss\r\n",
        "            val_h = net3.init_hidden(batch_size)\r\n",
        "            val_losses = []\r\n",
        "            net3.eval()\r\n",
        "            for inputs, labels in valid_loader3:\r\n",
        "\r\n",
        "\r\n",
        "                val_h = tuple([each.data for each in val_h])\r\n",
        "\r\n",
        "                if(train_on_gpu):\r\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "\r\n",
        "                inputs = inputs.type(torch.LongTensor)\r\n",
        "                inputs = inputs.cuda()\r\n",
        "                output, val_h = net3(inputs, val_h)\r\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\r\n",
        "\r\n",
        "                val_losses.append(val_loss.item())\r\n",
        "\r\n",
        "            net3.train()\r\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\r\n",
        "                  \"Step: {}...\".format(counter),\r\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\r\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQDGiEQvkKBd"
      },
      "source": [
        "Test du modèle 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knZ4OKLmj20J",
        "outputId": "39d6d252-f0ab-459e-ef45-605926e8bb30"
      },
      "source": [
        "# Get test data loss and accuracy\r\n",
        "\r\n",
        "test_losses = [] # track loss\r\n",
        "num_correct = 0\r\n",
        "\r\n",
        "# init hidden state\r\n",
        "h = net3.init_hidden(batch_size)\r\n",
        "\r\n",
        "net3.eval()\r\n",
        "# iterate over test data\r\n",
        "for inputs, labels in test_loader3:\r\n",
        "\r\n",
        "    h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "    if(train_on_gpu):\r\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "    \r\n",
        "    # get predicted outputs\r\n",
        "    inputs = inputs.type(torch.LongTensor)\r\n",
        "    inputs = inputs.cuda()\r\n",
        "    output, h = net3(inputs, h)\r\n",
        "    \r\n",
        "    # calculate loss\r\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\r\n",
        "    test_losses.append(test_loss.item())\r\n",
        "    \r\n",
        "    # convert output probabilities to predicted class (0 or 1)\r\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\r\n",
        "    \r\n",
        "    # compare predictions to true label\r\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\r\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\r\n",
        "    num_correct += np.sum(correct)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# accuracy over all test data\r\n",
        "test_acc = num_correct/len(test_loader.dataset)\r\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.480\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhfPbIx1kUem"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie90TWOXuW8d"
      },
      "source": [
        "# Evaluation des différents modèles dans leurs out-domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_EPUiILvcFT"
      },
      "source": [
        "**Premier modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Xe8qDKu1x9"
      },
      "source": [
        "# Get test data loss and accuracy\r\n",
        "\r\n",
        "test_losses = [] # track loss\r\n",
        "num_correct = 0\r\n",
        "\r\n",
        "# init hidden state\r\n",
        "h = net.init_hidden(batch_size)\r\n",
        "\r\n",
        "net.eval()\r\n",
        "# iterate over test data\r\n",
        "for inputs, labels in test_loader2:\r\n",
        "\r\n",
        "    # Creating new variables for the hidden state, otherwise\r\n",
        "    # we'd backprop through the entire training history\r\n",
        "    h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "    if(train_on_gpu):\r\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "    \r\n",
        "    # get predicted outputs\r\n",
        "    inputs = inputs.type(torch.LongTensor)\r\n",
        "    inputs = inputs.cuda()\r\n",
        "    output, h = net(inputs, h)\r\n",
        "    \r\n",
        "    # calculate loss\r\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\r\n",
        "    test_losses.append(test_loss.item())\r\n",
        "    \r\n",
        "    # convert output probabilities to predicted class (0 or 1)\r\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\r\n",
        "    \r\n",
        "    # compare predictions to true label\r\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\r\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\r\n",
        "    num_correct += np.sum(correct)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# accuracy over all test data\r\n",
        "test_acc = num_correct/len(test_loader.dataset)\r\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T9e_-_1vvzF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE4l52rTyqkO"
      },
      "source": [
        "**Deuxième modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaH5HBZyvpN"
      },
      "source": [
        "# Get test data loss and accuracy\r\n",
        "\r\n",
        "test_losses = [] # track loss\r\n",
        "num_correct = 0\r\n",
        "\r\n",
        "# init hidden state\r\n",
        "h = net2.init_hidden(batch_size)\r\n",
        "\r\n",
        "net2.eval()\r\n",
        "# iterate over test data\r\n",
        "for inputs, labels in test_loader1:\r\n",
        "\r\n",
        "    # Creating new variables for the hidden state, otherwise\r\n",
        "    # we'd backprop through the entire training history\r\n",
        "    h = tuple([each.data for each in h])\r\n",
        "\r\n",
        "    if(train_on_gpu):\r\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\r\n",
        "    \r\n",
        "    # get predicted outputs\r\n",
        "    inputs = inputs.type(torch.LongTensor)\r\n",
        "    inputs = inputs.cuda()\r\n",
        "    output, h = net2(inputs, h)\r\n",
        "    \r\n",
        "    # calculate loss\r\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\r\n",
        "    test_losses.append(test_loss.item())\r\n",
        "    \r\n",
        "    # convert output probabilities to predicted class (0 or 1)\r\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\r\n",
        "    \r\n",
        "    # compare predictions to true label\r\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\r\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\r\n",
        "    num_correct += np.sum(correct)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# accuracy over all test data\r\n",
        "test_acc = num_correct/len(test_loader.dataset)\r\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnkuPikVzHHd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}